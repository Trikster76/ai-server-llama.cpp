<img width="999" height="952" alt="llama" src="https://github.com/user-attachments/assets/eaae1c9c-7a32-45bc-be05-661038281edb" />
<img width="996" height="942" alt="llama1" src="https://github.com/user-attachments/assets/5e189366-8a74-415e-9f6c-737680b5dece" />
<img width="997" height="944" alt="llama2" src="https://github.com/user-attachments/assets/c122a806-b3b4-4e71-bad4-d863b8f8d595" />
<img width="993" height="994" alt="llama3" src="https://github.com/user-attachments/assets/e463536d-f6c0-4467-ab3c-7a05fa7dd6c2" />
# üöÄ AI Server Launcher

**Advanced GUI application for managing llama.cpp and Open WebUI servers with flexible parameter configuration and multilingual support**

[English](#english) | [–†—É—Å—Å–∫–∏–π](#—Ä—É—Å—Å–∫–∏–π)

---

## English

### Overview

**AI Server Launcher** is a powerful desktop application that simplifies the management of local AI language model servers. It provides an intuitive graphical interface for controlling:

- **llama.cpp Server** - High-performance inference engine for local LLM models
- **Open WebUI** - Feature-rich web interface for interacting with LLMs

### ‚ú® Key Features

- üéØ **Easy Server Management** - Start/stop servers with a single click
- ‚öôÔ∏è **Flexible Configuration** - Control all llama.cpp parameters from GUI
- üåç **Multilingual Support** - Russian and English interface languages
- üíæ **Smart Settings** - Automatic settings save/load per model
- üé® **Multiple Themes** - Dark (Darkly), Light (Litera), and High Contrast (Superhero) modes
- üß† **Intelligent Presets** - Auto-configuration for Chat, Instruct, Code, Story, Creative modes
- üìä **Real-time Logs** - Live monitoring of server output
- üöÄ **GPU Acceleration** - Full support for GPU-accelerated inference
- üîí **API Security** - Support for Bearer token authentication
- üñ•Ô∏è **Network Access** - Configure server accessibility (localhost or network-wide)

### üìã Screenshots

#### Main Interface
<img width="999" height="952" alt="llama" src="https://github.com/user-attachments/assets/eaae1c9c-7a32-45bc-be05-661038281edb" />

#### Generation Parameters
<img width="996" height="942" alt="llama1" src="https://github.com/user-attachments/assets/5e189366-8a74-415e-9f6c-737680b5dece" />

#### System Configuration
<img width="997" height="944" alt="llama2" src="https://github.com/user-attachments/assets/c122a806-b3b4-4e71-bad4-d863b8f8d595" />

#### Text Generation Parameters
<img width="993" height="994" alt="llama3" src="https://github.com/user-attachments/assets/e463536d-f6c0-4467-ab3c-7a05fa7dd6c2" />

### üîß System Requirements

- **Python**: 3.10 or higher (tested on 3.13.5)
- **Operating System**: Windows, macOS, Linux
- **RAM**: 8GB minimum (16GB+ recommended)
- **VRAM**: 2GB minimum (for GPU acceleration)

### üì¶ Dependencies

```
tkinter (included with Python)
ttkbootstrap >= 1.6.0
```

### üöÄ Quick Start

#### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Trikster76/ai-server-llama.cpp.git
   cd ai-server-llama.cpp
   ```

2. **Create virtual environment** (recommended)
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Download llama.cpp**
   - Download from [llama.cpp releases](https://github.com/ggerganov/llama.cpp/releases)
   - Extract to a convenient location

5. **Download a model**
   - Get GGUF format models from [Hugging Face](https://huggingface.co/models?search=gguf)
   - Popular choices: Llama 2, Mistral, Neural Chat

6. **Run the application**
   ```bash
   python AI-Server-Launcher.py
   ```

#### First Run

1. Open the application
2. Select **"Browse..."** to choose your `llama-server.exe` executable
3. Select **"Browse..."** to choose your GGUF model file
4. Configure parameters in the **"Main"** tab (port, context, GPU layers, threads)
5. Click **"Run Llama.cpp"** to start the server
6. Open WebUI or access via API at `http://127.0.0.1:8080/v1/chat/completions`

### ‚öôÔ∏è Advanced Configuration

#### Main Tab Parameters

| Parameter | Description | Default | Notes |
|-----------|-------------|---------|-------|
| **Port** | Server listening port | 8080 | Use different ports for multiple instances |
| **Context** | Token context window | 4096 | Larger = more memory usage |
| **GPU Layers** | Layers to offload to GPU | -1 | -1 = all layers, increase for better performance |
| **CPU Threads** | CPU threads for inference | 8 | Match your CPU core count |
| **Batch Size** | Prompt batch size | 512 | Higher = faster but more VRAM usage |

#### Generation Parameters (Sampling Tab)

| Parameter | Description | Default | Range | Notes |
|-----------|-------------|---------|-------|-------|
| **Temperature** | Response creativity | 0.7 | 0.0-2.0 | Lower = more deterministic |
| **Top-K** | Vocabulary filtering | 40 | 0-100 | Limits token selection to K most likely |
| **Top-P** | Nucleus sampling | 0.95 | 0.0-1.0 | Cumulative probability threshold |
| **Repeat Penalty** | Token repetition control | 1.1 | 1.0+ | Higher = less repetition |

#### Network Settings (System & Network Tab)

| Setting | Description | Default |
|---------|-------------|---------|
| **Host** | Binding address | 127.0.0.1 |
| **API Key** | Bearer token (optional) | - |
| **mlock** | Lock model in RAM | Enabled |

### üß† Smart Presets

The application includes intelligent presets that auto-configure parameters based on model type:

- **Chat** - Balanced settings for conversational AI
  - Temperature: 0.7, Top-K: 50, Top-P: 0.9
- **Instruct** - Precise following of instructions
  - Temperature: 0.2, Top-K: 40, Top-P: 0.95
- **Code** - Optimized for code generation
  - Temperature: 0.1, Top-K: 10, Top-P: 0.95, Repeat Penalty: 1.0
- **Story** - Creative narrative generation
  - Temperature: 0.9, Top-K: 0, Top-P: 0.9, Repeat Penalty: 1.15
- **Creative** - Maximum creativity
  - Temperature: 1.0, Top-K: 0, Top-P: 0.9, Repeat Penalty: 1.15

Simply select a model with matching keywords (e.g., `mistral-chat.gguf`) and click "Apply Model Preset".

### üåê API Usage

Once the server is running, you can interact with it via HTTP API:

#### Chat Completions
```bash
curl -X POST http://127.0.0.1:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default",
    "messages": [{"role": "user", "content": "Hello!"}],
    "temperature": 0.7,
    "max_tokens": 512
  }'
```

#### Embeddings
```bash
curl -X POST http://127.0.0.1:8080/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Your text here",
    "model": "default"
  }'
```

### üìñ Documentation Files

- **[FEATURES.md](docs/FEATURES.md)** - Detailed feature descriptions
- **[INSTALL.md](docs/INSTALL.md)** - Advanced installation guide
- **[API.md](docs/API.md)** - Complete API reference
- **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - Common issues and solutions

### üêõ Troubleshooting

**Q: "Llama.cpp executable not found"**
- A: Make sure you selected the correct `llama-server.exe` file

**Q: Server starts but crashes immediately**
- A: Check context size is less than model's max tokens, reduce GPU layers if out of VRAM

**Q: Poor performance on GPU**
- A: Increase GPU layers (use -1 for all), ensure GPU drivers are updated

**Q: Open WebUI won't connect**
- A: Verify port number is correct and firewall isn't blocking connections

See [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) for more solutions.

### ü§ù Contributing

Contributions are welcome! Please feel free to:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### üôè Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - High-performance LLM inference
- [Open WebUI](https://github.com/open-webui/open-webui) - Modern web interface
- [ttkbootstrap](https://github.com/israel-dryer/ttkbootstrap) - Modern Tkinter themes

### üí¨ Support

- üìß Issues: [GitHub Issues](https://github.com/Trikster76/ai-server-llama.cpp/issues)
- üí° Discussions: [GitHub Discussions](https://github.com/Trikster76/ai-server-llama.cpp/discussions)

---

## –†—É—Å—Å–∫–∏–π

### –û–ø–∏—Å–∞–Ω–∏–µ

**AI Server Launcher** ‚Äî —ç—Ç–æ –º–æ—â–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Ä–∞–±–æ—á–µ–≥–æ —Å—Ç–æ–ª–∞, –∫–æ—Ç–æ—Ä–æ–µ —É–ø—Ä–æ—â–∞–µ—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ —Å–µ—Ä–≤–µ—Ä–∞–º–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ò–ò. –û–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–π –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è:

- **–°–µ—Ä–≤–µ—Ä llama.cpp** ‚Äî –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM –º–æ–¥–µ–ª–µ–π
- **Open WebUI** ‚Äî –º–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM

### ‚ú® –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- üéØ **–ü—Ä–æ—Å—Ç–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ä–≤–µ—Ä–∞–º–∏** - –ó–∞–ø—É—Å–∫/–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–æ–≤ –æ–¥–Ω–∏–º –∫–ª–∏–∫–æ–º
- ‚öôÔ∏è **–ì–∏–±–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è** - –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ llama.cpp —á–µ—Ä–µ–∑ GUI
- üåç **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–≥–∏—Ö —è–∑—ã–∫–æ–≤** - –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
- üíæ **–£–º–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏** - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
- üé® **–ù–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º** - –¢—ë–º–Ω–∞—è (Darkly), —Å–≤–µ—Ç–ª–∞—è (Litera) –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–∞—è (Superhero)
- üß† **–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã** - –ê–≤—Ç–æ–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–µ–∂–∏–º–æ–≤ Chat, Instruct, Code, Story, Creative
- üìä **–ñ–∏–≤—ã–µ –ª–æ–≥–∏** - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–≤–æ–¥–∞ —Å–µ—Ä–≤–µ—Ä–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- üöÄ **–£—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ GPU** - –ü–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ GPU-—É—Å–∫–æ—Ä–µ–Ω–∏—è
- üîí **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å API** - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Bearer token
- üñ•Ô∏è **–î–æ—Å—Ç—É–ø –≤ —Å–µ—Ç–∏** - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞ (–ª–æ–∫–∞–ª—å–Ω–æ –∏–ª–∏ –ø–æ —Å–µ—Ç–∏)

### üîß –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- **Python**: 3.10 –∏–ª–∏ –≤—ã—à–µ (–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ 3.13.5)
- **–û–°**: Windows, macOS, Linux
- **–û–ó–£**: –º–∏–Ω–∏–º—É–º 8GB (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 16GB+)
- **VRAM**: –º–∏–Ω–∏–º—É–º 2GB (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ GPU)

### üì¶ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```
tkinter (–≤—Ö–æ–¥–∏—Ç –≤ Python)
ttkbootstrap >= 1.6.0
```

### üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

#### –£—Å—Ç–∞–Ω–æ–≤–∫–∞

1. **–ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π**
   ```bash
   git clone https://github.com/Trikster76/ai-server-llama.cpp.git
   cd ai-server-llama.cpp
   ```

2. **–°–æ–∑–¥–∞—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ** (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
   ```bash
   python -m venv venv
   source venv/bin/activate  # –ù–∞ Windows: venv\Scripts\activate
   ```

3. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**
   ```bash
   pip install -r requirements.txt
   ```

4. **–ó–∞–≥—Ä—É–∑–∏—Ç—å llama.cpp**
   - –°–∫–∞—á–∞–π—Ç–µ —Å [llama.cpp releases](https://github.com/ggerganov/llama.cpp/releases)
   - –†–∞—Å–ø–∞–∫—É–π—Ç–µ –≤ —É–¥–æ–±–Ω–æ–µ –º–µ—Å—Ç–æ

5. **–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å**
   - –ü–æ–ª—É—á–∏—Ç–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ GGUF —Å [Hugging Face](https://huggingface.co/models?search=gguf)
   - –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ: Llama 2, Mistral, Neural Chat

6. **–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ**
   ```bash
   python AI-Server-Launcher.py
   ```

#### –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫

1. –û—Ç–∫—Ä–æ–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
2. –ù–∞–∂–º–∏—Ç–µ **"–û–±–∑–æ—Ä..."** –∏ –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª `llama-server.exe`
3. –ù–∞–∂–º–∏—Ç–µ **"–û–±–∑–æ—Ä..."** –∏ –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ GGUF
4. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ —Ç–∞–±–µ **"–û—Å–Ω–æ–≤–Ω—ã–µ"** (–ø–æ—Ä—Ç, –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å–ª–æ–∏ GPU, –ø–æ—Ç–æ–∫–∏)
5. –ù–∞–∂–º–∏—Ç–µ **"–ó–∞–ø—É—Å—Ç–∏—Ç—å Llama.cpp"** –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
6. –û—Ç–∫—Ä–æ–π—Ç–µ WebUI –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ API –ø–æ –∞–¥—Ä–µ—Å—É `http://127.0.0.1:8080/v1/chat/completions`

### ‚öôÔ∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∫–ª–∞–¥–∫–∏ "–û—Å–Ω–æ–≤–Ω—ã–µ"

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –ü—Ä–∏–º–µ—á–∞–Ω–∏—è |
|----------|---------|--------------|-----------|
| **–ü–æ—Ä—Ç** | –ü–æ—Ä—Ç –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞ | 8080 | –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ |
| **–ö–æ–Ω—Ç–µ–∫—Å—Ç** | –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö | 4096 | –ë–æ–ª—å—à–µ = –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ |
| **–°–ª–æ–∏ –ì–ü–£** | –°–ª–æ–∏ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ –Ω–∞ GPU | -1 | -1 = –≤—Å–µ —Å–ª–æ–∏, —É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ |
| **–ü–æ—Ç–æ–∫–∏ –¶–ü–£** | –ü–æ—Ç–æ–∫–∏ –¶–ü–£ –¥–ª—è –≤—ã–≤–æ–¥–∞ | 8 | –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ä–∞–≤–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —è–¥–µ—Ä –¶–ü–£ |
| **–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞** | –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ | 512 | –ë–æ–ª—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –±–æ–ª—å—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM |

#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–≤–∫–ª–∞–¥–∫–∞ "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è")

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –î–∏–∞–ø–∞–∑–æ–Ω | –ü—Ä–∏–º–µ—á–∞–Ω–∏—è |
|----------|---------|--------------|----------|-----------|
| **–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞** | –¢–≤–æ—Ä—á–µ—Å–∫–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ | 0.7 | 0.0-2.0 | –ù–∏–∂–µ = –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ |
| **Top-K** | –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–æ–≤–∞—Ä—è | 40 | 0-100 | –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—ã–±–æ—Ä K —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ |
| **Top-P** | –í—ã–±–æ—Ä–∫–∞ —è–¥—Ä–∞ | 0.95 | 0.0-1.0 | –ü–æ—Ä–æ–≥ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ |
| **–®—Ç—Ä–∞—Ñ –ø–æ–≤—Ç–æ—Ä–∞** | –ö–æ–Ω—Ç—Ä–æ–ª—å –ø–æ–≤—Ç–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ | 1.1 | 1.0+ | –í—ã—à–µ = –º–µ–Ω—å—à–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π |

#### –°–µ—Ç–µ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–≤–∫–ª–∞–¥–∫–∞ "–°–∏—Å—Ç–µ–º–∞ –∏ –°–µ—Ç—å")

| –ù–∞—Å—Ç—Ä–æ–π–∫–∞ | –û–ø–∏—Å–∞–Ω–∏–µ | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é |
|-----------|---------|--------------|
| **–•–æ—Å—Ç** | –ê–¥—Ä–µ—Å –ø—Ä–∏–≤—è–∑–∫–∏ | 127.0.0.1 |
| **–ö–ª—é—á API** | Bearer token (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) | - |
| **mlock** | –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –û–ó–£ | –í–∫–ª—é—á–µ–Ω–æ |

### üß† –£–º–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã

–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ—Å–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏:

- **Chat** - –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥–∏–∞–ª–æ–≥–∞
  - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: 0.7, Top-K: 50, Top-P: 0.9
- **Instruct** - –¢–æ—á–Ω–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º
  - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: 0.2, Top-K: 40, Top-P: 0.95
- **Code** - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞
  - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: 0.1, Top-K: 10, Top-P: 0.95, –®—Ç—Ä–∞—Ñ: 1.0
- **Story** - –¢–≤–æ—Ä—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
  - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: 0.9, Top-K: 0, Top-P: 0.9, –®—Ç—Ä–∞—Ñ: 1.15
- **Creative** - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ—Å—Ç—å
  - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: 1.0, Top-K: 0, Top-P: 0.9, –®—Ç—Ä–∞—Ñ: 1.15

–ü—Ä–æ—Å—Ç–æ –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, `mistral-chat.gguf`) –∏ –Ω–∞–∂–º–∏—Ç–µ "–ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä–µ—Å–µ—Ç –º–æ–¥–µ–ª–∏".

### üåê API –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

–ö–æ–≥–¥–∞ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω, –≤—ã –º–æ–∂–µ—Ç–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –Ω–∏–º —á–µ—Ä–µ–∑ HTTP API:

#### Chat Completions
```bash
curl -X POST http://127.0.0.1:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default",
    "messages": [{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç!"}],
    "temperature": 0.7,
    "max_tokens": 512
  }'
```

#### Embeddings
```bash
curl -X POST http://127.0.0.1:8080/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": "–í–∞—à —Ç–µ–∫—Å—Ç –∑–¥–µ—Å—å",
    "model": "default"
  }'
```

### üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- **[FEATURES.md](docs/FEATURES.md)** - –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
- **[INSTALL.md](docs/INSTALL.md)** - –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
- **[API.md](docs/API.md)** - –ü–æ–ª–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ API
- **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - –†–µ—à–µ–Ω–∏–µ —Ç–∏–ø–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º

### üêõ –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

**–í: "–ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª Llama.cpp –Ω–µ –Ω–∞–π–¥–µ–Ω"**
- –û: –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã–±—Ä–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–∞–π–ª `llama-server.exe`

**–í: –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–æ —Å—Ä–∞–∑—É –∫—Ä–∞—à–∏—Ç—Å—è**
- –û: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–µ–Ω—å—à–µ –º–∞–∫—Å–∏–º—É–º–∞ –º–æ–¥–µ–ª–∏, —É–º–µ–Ω—å—à–∏—Ç–µ —Å–ª–æ–∏ GPU –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM

**–í: –ü–ª–æ—Ö–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ GPU**
- –û: –£–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ GPU (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ -1 –¥–ª—è –≤—Å–µ—Ö), –æ–±–Ω–æ–≤–∏—Ç–µ –¥—Ä–∞–π–≤–µ—Ä—ã GPU

**–í: Open WebUI –Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è**
- –û: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –Ω–æ–º–µ—Ä–∞ –ø–æ—Ä—Ç–∞ –∏ —á—Ç–æ –±—Ä–∞–Ω–¥–º–∞—É—ç—Ä –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è

–°–º–æ—Ç—Ä–∏—Ç–µ [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π.

### ü§ù –í–Ω–µ—Å–µ–Ω–∏–µ –≤–∫–ª–∞–¥–∞

–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é—Ç—Å—è –∫–æ–Ω—Ç—Ä–∏–±—å—é—Ü–∏–∏! –í—ã –º–æ–∂–µ—Ç–µ:

1. –°–¥–µ–ª–∞—Ç—å fork —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
2. –°–æ–∑–¥–∞—Ç—å –≤–µ—Ç–∫—É –¥–ª—è –≤–∞—à–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ (`git checkout -b feature/amazing-feature`)
3. –°–¥–µ–ª–∞—Ç—å commit –∏–∑–º–µ–Ω–µ–Ω–∏–π (`git commit -m 'Add amazing feature'`)
4. –û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤–µ—Ç–∫—É (`git push origin feature/amazing-feature`)
5. –û—Ç–∫—Ä—ã—Ç—å Pull Request

### üìù –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–¥ –ª–∏—Ü–µ–Ω–∑–∏–µ–π MIT - —Å–º–æ—Ç—Ä–∏—Ç–µ —Ñ–∞–π–ª [LICENSE](LICENSE) –¥–ª—è –¥–µ—Ç–∞–ª–µ–π.

### üôè –°–ø–∞—Å–∏–±–æ

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ LLM
- [Open WebUI](https://github.com/open-webui/open-webui) - –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- [ttkbootstrap](https://github.com/israel-dryer/ttkbootstrap) - –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ–º—ã Tkinter

### üí¨ –ü–æ–¥–¥–µ—Ä–∂–∫–∞

- üìß Issues: [GitHub Issues](https://github.com/Trikster76/ai-server-llama.cpp/issues)
- üí° Discussions: [GitHub Discussions](https://github.com/Trikster76/ai-server-llama.cpp/discussions)

---

**Made with ‚ù§Ô∏è by [Trikster76](https://github.com/Trikster76)**
